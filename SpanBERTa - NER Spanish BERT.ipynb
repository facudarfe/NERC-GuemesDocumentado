{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"SpanBERTa - NER Spanish BERT.ipynb","provenance":[{"file_id":"1ezuE7wC7Fa21Wu3fvzRffx2m14CAySS1","timestamp":1639544024609}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"CfVkuWAW5r36"},"source":["# SpanBERTa: Reconocimiento de Entidades Nombradas con Transformers\n","##https://github.com/chriskhanhtran/spanish-bert"]},{"cell_type":"markdown","metadata":{"id":"R0rpgtxmkET4"},"source":["# Introducción"]},{"cell_type":"markdown","metadata":{"id":"LhKZ3vItVBzi"},"source":["SpanBERTa es un modelo de lenguaje basado en tecnología Transformer para español, entrenado desde cero sobre un gran corpus. En esta notebook, llevaremos a cabo un proceso de fine-tuning sobre SpanBERTa para la tarea de Reconocimiento de Entidades Nombradas.\n","\n","Utilizamos el script `run_ner.py` (dentro de la carpeta drive) y el dataset Güemes Documentado para el fine-tuning."]},{"cell_type":"markdown","metadata":{"id":"SfBvd7zVIbDc"},"source":["# Setup"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)\n","CARPETA_COLAB = '/content/drive/MyDrive/ProyectoNERC/NERC/'"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9RIj3VeM0Wo-","executionInfo":{"status":"ok","timestamp":1652804421566,"user_tz":180,"elapsed":21792,"user":{"displayName":"Facu Darfe","userId":"05435650411451715325"}},"outputId":"c3c0bc51-f2b4-4eea-9c6b-29fb5e6d04ed"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","metadata":{"id":"utGQ7w2Jjdi6"},"source":["Instalamos paquetes e importamos."]},{"cell_type":"code","source":["!pip install transformers\n","!pip install seqeval"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BsYmIOV_puEy","executionInfo":{"status":"ok","timestamp":1652804302701,"user_tz":180,"elapsed":22331,"user":{"displayName":"Facu Darfe","userId":"05435650411451715325"}},"outputId":"b3f8d6a3-afbf-486b-c2c6-ce2a29958edc"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting transformers\n","  Downloading transformers-4.19.2-py3-none-any.whl (4.2 MB)\n","\u001b[K     |████████████████████████████████| 4.2 MB 5.1 MB/s \n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Collecting huggingface-hub<1.0,>=0.1.0\n","  Downloading huggingface_hub-0.6.0-py3-none-any.whl (84 kB)\n","\u001b[K     |████████████████████████████████| 84 kB 2.7 MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.6.0)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n","  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n","\u001b[K     |████████████████████████████████| 6.6 MB 34.7 MB/s \n","\u001b[?25hCollecting pyyaml>=5.1\n","  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n","\u001b[K     |████████████████████████████████| 596 kB 37.5 MB/s \n","\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.2.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.8)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Installing collected packages: pyyaml, tokenizers, huggingface-hub, transformers\n","  Attempting uninstall: pyyaml\n","    Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","Successfully installed huggingface-hub-0.6.0 pyyaml-6.0 tokenizers-0.12.1 transformers-4.19.2\n","Collecting seqeval\n","  Downloading seqeval-1.2.2.tar.gz (43 kB)\n","\u001b[K     |████████████████████████████████| 43 kB 70 kB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from seqeval) (1.21.6)\n","Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.7/dist-packages (from seqeval) (1.0.2)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.1.0)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval) (3.1.0)\n","Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.4.1)\n","Building wheels for collected packages: seqeval\n","  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16180 sha256=7fd7c659f76780cf30cad693ba2a8290fcb7c5372c00fd3c7972a77fb9a322a3\n","  Stored in directory: /root/.cache/pip/wheels/05/96/ee/7cac4e74f3b19e3158dce26a20a1c86b3533c43ec72a549fd7\n","Successfully built seqeval\n","Installing collected packages: seqeval\n","Successfully installed seqeval-1.2.2\n"]}]},{"cell_type":"code","source":["import transformers"],"metadata":{"id":"rjsjkizSsgey","executionInfo":{"status":"ok","timestamp":1652804322185,"user_tz":180,"elapsed":10837,"user":{"displayName":"Facu Darfe","userId":"05435650411451715325"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GU8R8FeAIT2C"},"source":["# Datos\n"]},{"cell_type":"markdown","metadata":{"id":"JELuvmOKJaq6"},"source":["## 1. Cargamos Dataset"]},{"cell_type":"code","metadata":{"id":"bZSnJqZ2Sz4B","executionInfo":{"status":"ok","timestamp":1652806058627,"user_tz":180,"elapsed":342,"user":{"displayName":"Facu Darfe","userId":"05435650411451715325"}}},"source":["tokens = []\n","with open(CARPETA_COLAB+'dataset/GD-1.conll', 'r', encoding=\"utf-8\") as f:\n","  for line in f.readlines():\n","    splitted_text = line.split()\n","    if len(splitted_text) == 3:\n","      tokens.append((splitted_text[0], splitted_text[2]))"],"execution_count":30,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NQXQAFDg7ZpP"},"source":["Train y test (a y b) para Güemes Documentado\n","   - testa: Datos de prueba en español para la fase development\n","   - testb: Datos de prueba final\n","   - train: Datos de entrenamiento"]},{"cell_type":"code","source":["# Division 60 - 20 - 20. Se realiza a mano para asegurar que cada set termine en una oracion\n","train = tokens[:31020]\n","testa = tokens[31020:41395]\n","testb = tokens[41395:]"],"metadata":{"id":"Zxt03gK31pQ0","executionInfo":{"status":"ok","timestamp":1652806061586,"user_tz":180,"elapsed":2,"user":{"displayName":"Facu Darfe","userId":"05435650411451715325"}}},"execution_count":31,"outputs":[]},{"cell_type":"code","source":["with open(\"train_temp.txt\", 'w', encoding=\"utf-8\") as f:\n","  for token in train:\n","    f.write(token[0] + ' ' + token[1] + '\\n')\n","\n","with open(\"dev_temp.txt\", 'w', encoding=\"utf-8\") as f:\n","  for token in testa:\n","    f.write(token[0] + ' ' + token[1] + '\\n')\n","\n","with open(\"test_temp.txt\", 'w', encoding=\"utf-8\") as f:\n","  for token in testb:\n","    f.write(token[0] + ' ' + token[1] + '\\n')"],"metadata":{"id":"uwx6PDll4Ff8","executionInfo":{"status":"ok","timestamp":1652806264188,"user_tz":180,"elapsed":4,"user":{"displayName":"Facu Darfe","userId":"05435650411451715325"}}},"execution_count":37,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"b3c4v9IKJVjL"},"source":["## 2. Preprocesamiento"]},{"cell_type":"code","metadata":{"id":"Z4Wl3iR0UBZv","executionInfo":{"status":"ok","timestamp":1652806489650,"user_tz":180,"elapsed":4,"user":{"displayName":"Facu Darfe","userId":"05435650411451715325"}}},"source":["MAX_LENGTH = 120 #@param {type: \"integer\"}\n","MODEL = \"chriskhanhtran/spanberta\" #@param [\"chriskhanhtran/spanberta\", \"bert-base-multilingual-cased\"]"],"execution_count":38,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hSfMYC7w_UpX"},"source":["Acortamos oraciones a un máximo (`MAX_LENGTH`) en cantidad de tokens."]},{"cell_type":"code","metadata":{"id":"3gmazeojxn80","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1652806498216,"user_tz":180,"elapsed":298,"user":{"displayName":"Facu Darfe","userId":"05435650411451715325"}},"outputId":"1326f308-2c0d-45af-abcf-a1d2ef85043f"},"source":["#%%capture\n","!wget \"https://raw.githubusercontent.com/stefan-it/fine-tuned-berts-seq/master/scripts/preprocess.py\""],"execution_count":39,"outputs":[{"output_type":"stream","name":"stdout","text":["--2022-05-17 16:54:58--  https://raw.githubusercontent.com/stefan-it/fine-tuned-berts-seq/master/scripts/preprocess.py\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.111.133, 185.199.109.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 991 [text/plain]\n","Saving to: ‘preprocess.py’\n","\n","\rpreprocess.py         0%[                    ]       0  --.-KB/s               \rpreprocess.py       100%[===================>]     991  --.-KB/s    in 0s      \n","\n","2022-05-17 16:54:58 (46.9 MB/s) - ‘preprocess.py’ saved [991/991]\n","\n"]}]},{"cell_type":"code","metadata":{"id":"qn10UkLQxNg3","outputId":"5651bd7a-2a3f-4426-bd38-926e77d531e8","executionInfo":{"status":"ok","timestamp":1652806536759,"user_tz":180,"elapsed":27606,"user":{"displayName":"Facu Darfe","userId":"05435650411451715325"}},"colab":{"base_uri":"https://localhost:8080/"}},"source":["!python3 preprocess.py train_temp.txt $MODEL $MAX_LENGTH > train.txt\n","!python3 preprocess.py dev_temp.txt $MODEL $MAX_LENGTH > dev.txt\n","!python3 preprocess.py test_temp.txt $MODEL $MAX_LENGTH > test.txt"],"execution_count":40,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading: 100% 16.0/16.0 [00:00<00:00, 15.1kB/s]\n","Downloading: 100% 487/487 [00:00<00:00, 293kB/s]\n","Downloading: 100% 932k/932k [00:00<00:00, 5.70MB/s]\n","Downloading: 100% 500k/500k [00:00<00:00, 3.44MB/s]\n"]}]},{"cell_type":"markdown","metadata":{"id":"_SJQJHNlUNMP"},"source":["## 3. Etiquetas\n","\n","En los datasets CoNLL-2002/2003, hay 9 clases de tags NER:"]},{"cell_type":"markdown","metadata":{"id":"d-DlGoiJLqAR"},"source":["- O, Fuera de entidad\n","- B-MIS, Inicio de entidad tipo miscelanea justo después de otra entidad miscelanea\n","- I-MIS, Entidad miscelanea\n","- B-PER, Inicio de entidad tipo persona justo después de otra entidad persona\n","- I-PER, Nombre de persona\n","- B-ORG, Inicio de entidad tipo organización justo después de otra entidad organización\n","- I-ORG, Organización\n","- B-LOC, Inicio de entidad tipo lugar justo después de otra entidad lugar\n","- I-LOC, lugar"]},{"cell_type":"markdown","metadata":{"id":"ee3mlWe2BQPZ"},"source":["Si existen más etiquetas en el dataset, la línea de abajo permite obtener los tags correspondientes al dataset actual. Los mismos se guardan en `labels.txt`, el cual se usará para el fine-tuning."]},{"cell_type":"code","metadata":{"id":"mEtntk5-Wnvu","executionInfo":{"status":"ok","timestamp":1652806573566,"user_tz":180,"elapsed":278,"user":{"displayName":"Facu Darfe","userId":"05435650411451715325"}}},"source":["!cat train.txt dev.txt test.txt | cut -d \" \" -f 2 | grep -v \"^$\"| sort | uniq > labels.txt"],"execution_count":41,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"G80k0O2aWwzy"},"source":["# Fine-tuning del modelo\n"]},{"cell_type":"markdown","metadata":{"id":"8Qy9XdmlJXqE"},"source":["Scripts del repositorio `transformers` a usar para el fine-tuning para NER. Luego del 21/04/2020, Hugging Face actualizó sus ejemplos para usar una nueva clase `Trainer`. Para evitar conflictos, se usa la versión anterior a estas actualizaciones."]},{"cell_type":"code","metadata":{"id":"1NoEUI-EGDQp","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1652806636207,"user_tz":180,"elapsed":328,"user":{"displayName":"Facu Darfe","userId":"05435650411451715325"}},"outputId":"3a6b625f-a00e-4690-f530-9cfb5ec75a4a"},"source":["#%%capture\n","#No usamos run_ner.py del repo original por errores con las nuevas versiones de transformers\n","#!wget \"https://raw.githubusercontent.com/chriskhanhtran/spanish-bert/master/ner/run_ner.py\"\n","!wget \"https://raw.githubusercontent.com/chriskhanhtran/spanish-bert/master/ner/utils_ner.py\""],"execution_count":42,"outputs":[{"output_type":"stream","name":"stdout","text":["--2022-05-17 16:57:16--  https://raw.githubusercontent.com/chriskhanhtran/spanish-bert/master/ner/utils_ner.py\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.108.133, 185.199.111.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 8603 (8.4K) [text/plain]\n","Saving to: ‘utils_ner.py’\n","\n","\rutils_ner.py          0%[                    ]       0  --.-KB/s               \rutils_ner.py        100%[===================>]   8.40K  --.-KB/s    in 0s      \n","\n","2022-05-17 16:57:16 (41.2 MB/s) - ‘utils_ner.py’ saved [8603/8603]\n","\n"]}]},{"cell_type":"markdown","metadata":{"id":"Xefzt_b1IiQb"},"source":["Fase de `Transfer Learning`. [Aquí](https://chriskhanhtran.github.io/posts/spanberta-bert-for-spanish-from-scratch/), se puede ver un ejemplo de pre-entrenamiento de un modelo RoBERTa en un corpus gigantesco en español, para predicción de palabras enmascaradas (masked words). Mediante este proceso se obtiene un modelo que aprendió propiedades básicas del lenguaje.\n","\n","Se pueden observar abajo los hiperparámetros del modelo."]},{"cell_type":"code","metadata":{"id":"rR7lmegy5Pzq","executionInfo":{"status":"ok","timestamp":1652806719260,"user_tz":180,"elapsed":267,"user":{"displayName":"Facu Darfe","userId":"05435650411451715325"}}},"source":["MAX_LENGTH = 128 #@param {type: \"integer\"}\n","MODEL = \"chriskhanhtran/spanberta\" #@param [\"chriskhanhtran/spanberta\", \"bert-base-multilingual-cased\"]\n","OUTPUT_DIR = \"spanberta-ner\" #@param [\"spanberta-ner\", \"bert-base-ml-ner\"]\n","BATCH_SIZE = 32 #@param {type: \"integer\"}\n","NUM_EPOCHS = 3 #@param {type: \"integer\"}\n","SAVE_STEPS = 100 #@param {type: \"integer\"}\n","LOGGING_STEPS = 100 #@param {type: \"integer\"}\n","SEED = 42 #@param {type: \"integer\"}"],"execution_count":43,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jYATwvIjG-WN"},"source":["Inicio del entrenamiento."]},{"cell_type":"code","metadata":{"id":"AG-f9zzGWzWz","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1652807364006,"user_tz":180,"elapsed":71523,"user":{"displayName":"Facu Darfe","userId":"05435650411451715325"}},"outputId":"4686b51a-f4ff-4de1-e00e-58390e512990"},"source":["!python3 '/content/drive/MyDrive/ProyectoNERC/NERC/run_ner.py' \\\n","  --data_dir ./ \\\n","  --model_type bert \\\n","  --labels ./labels.txt \\\n","  --model_name_or_path $MODEL \\\n","  --output_dir $OUTPUT_DIR \\\n","  --max_seq_length  $MAX_LENGTH \\\n","  --num_train_epochs $NUM_EPOCHS \\\n","  --per_gpu_train_batch_size $BATCH_SIZE \\\n","  --save_steps $SAVE_STEPS \\\n","  --logging_steps $LOGGING_STEPS \\\n","  --seed $SEED \\\n","  --do_train \\\n","  --do_eval \\\n","  --do_predict \\\n","  --overwrite_output_dir"],"execution_count":48,"outputs":[{"output_type":"stream","name":"stdout","text":["05/17/2022 17:08:16 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n","05/17/2022 17:08:16 - INFO - __main__ -   Tokenizer arguments: {'do_lower_case': False}\n","Some weights of the model checkpoint at chriskhanhtran/spanberta were not used when initializing RobertaForTokenClassification: ['lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.decoder.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight']\n","- This IS expected if you are initializing RobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at chriskhanhtran/spanberta and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","05/17/2022 17:08:23 - INFO - __main__ -   Training/evaluation parameters Namespace(adam_epsilon=1e-08, cache_dir='', config_name='', data_dir='./', device=device(type='cuda'), do_eval=True, do_lower_case=False, do_predict=True, do_train=True, eval_all_checkpoints=False, evaluate_during_training=False, fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=1, keep_accents=None, labels='./labels.txt', learning_rate=5e-05, local_rank=-1, logging_steps=100, max_grad_norm=1.0, max_seq_length=128, max_steps=-1, model_name_or_path='chriskhanhtran/spanberta', model_type='bert', n_gpu=1, no_cuda=False, num_train_epochs=3.0, output_dir='spanberta-ner', overwrite_cache=False, overwrite_output_dir=True, per_gpu_eval_batch_size=8, per_gpu_train_batch_size=32, save_steps=100, seed=42, server_ip='', server_port='', strip_accents=None, tokenizer_name='', use_fast=None, warmup_steps=0, weight_decay=0.0)\n","05/17/2022 17:08:23 - INFO - __main__ -   Loading features from cached file ./cached_train_spanberta_128\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","05/17/2022 17:08:23 - INFO - __main__ -   ***** Running training *****\n","05/17/2022 17:08:23 - INFO - __main__ -     Num examples = 392\n","05/17/2022 17:08:23 - INFO - __main__ -     Num Epochs = 3\n","05/17/2022 17:08:23 - INFO - __main__ -     Instantaneous batch size per GPU = 32\n","05/17/2022 17:08:23 - INFO - __main__ -     Total train batch size (w. parallel, distributed & accumulation) = 32\n","05/17/2022 17:08:23 - INFO - __main__ -     Gradient Accumulation steps = 1\n","05/17/2022 17:08:23 - INFO - __main__ -     Total optimization steps = 39\n","Epoch:   0% 0/3 [00:00<?, ?it/s]\n","Iteration:   0% 0/13 [00:00<?, ?it/s]\u001b[A\n","Iteration:   8% 1/13 [00:01<00:16,  1.38s/it]\u001b[A\n","Iteration:  15% 2/13 [00:02<00:14,  1.35s/it]\u001b[A\n","Iteration:  23% 3/13 [00:04<00:13,  1.35s/it]\u001b[A\n","Iteration:  31% 4/13 [00:05<00:12,  1.34s/it]\u001b[A\n","Iteration:  38% 5/13 [00:06<00:10,  1.34s/it]\u001b[A\n","Iteration:  46% 6/13 [00:08<00:09,  1.35s/it]\u001b[A\n","Iteration:  54% 7/13 [00:09<00:08,  1.35s/it]\u001b[A\n","Iteration:  62% 8/13 [00:10<00:06,  1.35s/it]\u001b[A\n","Iteration:  69% 9/13 [00:12<00:05,  1.35s/it]\u001b[A\n","Iteration:  77% 10/13 [00:13<00:04,  1.35s/it]\u001b[A\n","Iteration:  85% 11/13 [00:14<00:02,  1.35s/it]\u001b[A\n","Iteration:  92% 12/13 [00:16<00:01,  1.35s/it]\u001b[A\n","Iteration: 100% 13/13 [00:16<00:00,  1.28s/it]\n","Epoch:  33% 1/3 [00:16<00:33, 16.62s/it]\n","Iteration:   0% 0/13 [00:00<?, ?it/s]\u001b[A\n","Iteration:   8% 1/13 [00:01<00:16,  1.35s/it]\u001b[A\n","Iteration:  15% 2/13 [00:02<00:14,  1.36s/it]\u001b[A\n","Iteration:  23% 3/13 [00:04<00:13,  1.36s/it]\u001b[A\n","Iteration:  31% 4/13 [00:05<00:12,  1.36s/it]\u001b[A\n","Iteration:  38% 5/13 [00:06<00:10,  1.37s/it]\u001b[A\n","Iteration:  46% 6/13 [00:08<00:09,  1.37s/it]\u001b[A\n","Iteration:  54% 7/13 [00:09<00:08,  1.37s/it]\u001b[A\n","Iteration:  62% 8/13 [00:10<00:06,  1.37s/it]\u001b[A\n","Iteration:  69% 9/13 [00:12<00:05,  1.36s/it]\u001b[A\n","Iteration:  77% 10/13 [00:13<00:04,  1.36s/it]\u001b[A\n","Iteration:  85% 11/13 [00:15<00:02,  1.36s/it]\u001b[A\n","Iteration:  92% 12/13 [00:16<00:01,  1.36s/it]\u001b[A\n","Iteration: 100% 13/13 [00:16<00:00,  1.29s/it]\n","Epoch:  67% 2/3 [00:33<00:16, 16.74s/it]\n","Iteration:   0% 0/13 [00:00<?, ?it/s]\u001b[A\n","Iteration:   8% 1/13 [00:01<00:16,  1.36s/it]\u001b[A\n","Iteration:  15% 2/13 [00:02<00:15,  1.37s/it]\u001b[A\n","Iteration:  23% 3/13 [00:04<00:13,  1.37s/it]\u001b[A\n","Iteration:  31% 4/13 [00:05<00:12,  1.38s/it]\u001b[A\n","Iteration:  38% 5/13 [00:06<00:10,  1.37s/it]\u001b[A\n","Iteration:  46% 6/13 [00:08<00:09,  1.37s/it]\u001b[A\n","Iteration:  54% 7/13 [00:09<00:08,  1.37s/it]\u001b[A\n","Iteration:  62% 8/13 [00:10<00:06,  1.36s/it]\u001b[A\n","Iteration:  69% 9/13 [00:12<00:05,  1.36s/it]\u001b[A\n","Iteration:  77% 10/13 [00:13<00:04,  1.36s/it]\u001b[A\n","Iteration:  85% 11/13 [00:14<00:02,  1.36s/it]\u001b[A\n","Iteration:  92% 12/13 [00:16<00:01,  1.36s/it]\u001b[A\n","Iteration: 100% 13/13 [00:16<00:00,  1.29s/it]\n","Epoch: 100% 3/3 [00:50<00:00, 16.75s/it]\n","05/17/2022 17:09:13 - INFO - __main__ -    global_step = 39, average loss = 0.33866021495599014\n","05/17/2022 17:09:13 - INFO - __main__ -   Saving model checkpoint to spanberta-ner\n","05/17/2022 17:09:15 - INFO - __main__ -   Evaluate the following checkpoints: ['spanberta-ner']\n","05/17/2022 17:09:17 - INFO - __main__ -   Loading features from cached file ./cached_dev_spanberta_128\n","05/17/2022 17:09:17 - INFO - __main__ -   ***** Running evaluation  *****\n","05/17/2022 17:09:17 - INFO - __main__ -     Num examples = 135\n","05/17/2022 17:09:17 - INFO - __main__ -     Batch size = 8\n","Evaluating: 100% 17/17 [00:02<00:00,  8.34it/s]\n","05/17/2022 17:09:19 - INFO - __main__ -   ***** Eval results  *****\n","05/17/2022 17:09:19 - INFO - __main__ -     f1 = 0.7103139013452915\n","05/17/2022 17:09:19 - INFO - __main__ -     loss = 0.10346653049483019\n","05/17/2022 17:09:19 - INFO - __main__ -     precision = 0.673469387755102\n","05/17/2022 17:09:19 - INFO - __main__ -     recall = 0.7514231499051234\n","05/17/2022 17:09:20 - INFO - __main__ -   Loading features from cached file ./cached_test_spanberta_128\n","05/17/2022 17:09:20 - INFO - __main__ -   ***** Running evaluation  *****\n","05/17/2022 17:09:20 - INFO - __main__ -     Num examples = 134\n","05/17/2022 17:09:20 - INFO - __main__ -     Batch size = 8\n","Evaluating: 100% 17/17 [00:02<00:00,  8.45it/s]\n","05/17/2022 17:09:22 - INFO - __main__ -   ***** Eval results  *****\n","05/17/2022 17:09:22 - INFO - __main__ -     f1 = 0.687557603686636\n","05/17/2022 17:09:22 - INFO - __main__ -     loss = 0.12104724084629732\n","05/17/2022 17:09:22 - INFO - __main__ -     precision = 0.7242718446601941\n","05/17/2022 17:09:22 - INFO - __main__ -     recall = 0.6543859649122807\n"]}]},{"cell_type":"markdown","metadata":{"id":"Go3OZkoeQ0yf"},"source":["Performance en el dev set:\n","```\n","21/04/2020 02:24:31 - INFO - __main__ -   ***** Eval results  *****\n","21/04/2020 02:24:31 - INFO - __main__ -     f1 = 0.831027443864822\n","21/04/2020 02:24:31 - INFO - __main__ -     loss = 0.1004064822183894\n","21/04/2020 02:24:31 - INFO - __main__ -     precision = 0.8207885304659498\n","21/04/2020 02:24:31 - INFO - __main__ -     recall = 0.8415250344510795\n","```\n","Performance en el test set:\n","```\n","21/04/2020 02:24:48 - INFO - __main__ -   ***** Eval results  *****\n","21/04/2020 02:24:48 - INFO - __main__ -     f1 = 0.8559533721898419\n","21/04/2020 02:24:48 - INFO - __main__ -     loss = 0.06848683688204177\n","21/04/2020 02:24:48 - INFO - __main__ -     precision = 0.845858475041141\n","21/04/2020 02:24:48 - INFO - __main__ -     recall = 0.8662921348314607\n","```"]},{"cell_type":"markdown","metadata":{"id":"ah2Of3XckcVm"},"source":["Gráficas de tensorboard del proceso de fine-tuning sobre [spanberta](https://tensorboard.dev/experiment/Ggs7aCjWQ0exU2Nbp3pPlQ/#scalars&_smoothingWeight=0.265) y [bert-base-multilingual-cased](https://tensorboard.dev/experiment/M9AXw2lORjeRzFZzEJOxkA/#scalars) para 5 épocas. Vemos que se produce overfitting luego de 3 épocas.\n","\n","![](https://raw.githubusercontent.com/chriskhanhtran/spanish-bert/master/img/spanberta-ner-tb-5.JPG)\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"8uAzVHU1WjaD"},"source":["**Reporte de Clasificación**"]},{"cell_type":"code","metadata":{"id":"lqU8mvjIS_4D","executionInfo":{"status":"ok","timestamp":1652807377192,"user_tz":180,"elapsed":401,"user":{"displayName":"Facu Darfe","userId":"05435650411451715325"}}},"source":["def read_examples_from_file(file_path):\n","    \"\"\"Read words and labels from a CoNLL-2002/2003 data file.\n","    \n","    Args:\n","      file_path (str): path to NER data file.\n","\n","    Returns:\n","      examples (dict): a dictionary with two keys: `words` (list of lists)\n","        holding words in each sequence, and `labels` (list of lists) holding\n","        corresponding labels.\n","    \"\"\"\n","    with open(file_path, encoding=\"utf-8\") as f:\n","        examples = {\"words\": [], \"labels\": []}\n","        words = []\n","        labels = []\n","        for line in f:\n","            if line.startswith(\"-DOCSTART-\") or line == \"\" or line == \"\\n\":\n","                if words:\n","                    examples[\"words\"].append(words)\n","                    examples[\"labels\"].append(labels)\n","                    words = []\n","                    labels = []\n","            else:\n","                splits = line.split(\" \")\n","                words.append(splits[0])\n","                if len(splits) > 1:\n","                    labels.append(splits[-1].replace(\"\\n\", \"\"))\n","                else:\n","                    # Examples could have no label for mode = \"test\"\n","                    labels.append(\"O\")\n","    return examples"],"execution_count":49,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Hizogs3MCpFp"},"source":["Cargamos datos y etiquetas de los textos originales:"]},{"cell_type":"code","metadata":{"id":"V9lVfi_A_Eln","executionInfo":{"status":"ok","timestamp":1652807380471,"user_tz":180,"elapsed":403,"user":{"displayName":"Facu Darfe","userId":"05435650411451715325"}}},"source":["y_true = read_examples_from_file(\"test.txt\")[\"labels\"]\n","y_pred = read_examples_from_file(\"spanberta-ner/test_predictions.txt\")[\"labels\"]"],"execution_count":50,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"59B5FFb_CvBp"},"source":["Reporte:"]},{"cell_type":"code","metadata":{"id":"lP98_LWEa5x1","outputId":"30532902-db98-4f2b-b03b-4941a0be3198","executionInfo":{"status":"ok","timestamp":1652807383253,"user_tz":180,"elapsed":299,"user":{"displayName":"Facu Darfe","userId":"05435650411451715325"}},"colab":{"base_uri":"https://localhost:8080/"}},"source":["from seqeval.metrics import classification_report as classification_report_seqeval\n","\n","print(classification_report_seqeval(y_true, y_pred))"],"execution_count":51,"outputs":[{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","        DATE       0.66      0.65      0.66        97\n","         LOC       0.79      0.62      0.69       226\n","         PER       0.70      0.69      0.69       246\n","\n","   micro avg       0.72      0.65      0.69       569\n","   macro avg       0.72      0.65      0.68       569\n","weighted avg       0.73      0.65      0.69       569\n","\n"]}]},{"cell_type":"markdown","metadata":{"id":"QyTHymw2ClDq"},"source":["Las métricas de este reporte están diseñadas específicamente para tareas NLP como NER y POS tagging, en las cuales todas las palabras de una entidad deben ser detectadas correctamente para contar como caso de éxito. Por ello los valores son más bajos que los del [reporte de clasificación de scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html)."]},{"cell_type":"code","metadata":{"id":"Li66swRquHmm","outputId":"99cc0464-dcf4-4cd4-d653-c551a3949adf","executionInfo":{"status":"ok","timestamp":1652807410785,"user_tz":180,"elapsed":309,"user":{"displayName":"Facu Darfe","userId":"05435650411451715325"}},"colab":{"base_uri":"https://localhost:8080/"}},"source":["import numpy as np\n","from sklearn.metrics import classification_report\n","\n","print(classification_report(np.concatenate(y_true), np.concatenate(y_pred)))"],"execution_count":52,"outputs":[{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","      B-DATE       0.74      0.67      0.70        90\n","       B-LOC       0.96      0.58      0.72       224\n","       B-PER       0.80      0.73      0.76       241\n","      I-DATE       0.97      0.82      0.89       206\n","       I-LOC       0.60      0.72      0.66        80\n","       I-PER       0.85      0.84      0.85       313\n","           O       0.98      0.99      0.99      9134\n","\n","    accuracy                           0.96     10288\n","   macro avg       0.84      0.76      0.80     10288\n","weighted avg       0.96      0.96      0.96     10288\n","\n"]}]}]}